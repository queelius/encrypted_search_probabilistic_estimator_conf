\documentclass[ ../main.tex]{subfiles}
\providecommand{\mainx}{..}
\begin{document}
\section{Mapping \emph{entropy} to \emph{confidentiality}}
\label{sec:sophisticated}
The adversary described in \Cref{sec:threatmodel} may efficiently compromise the confidentiality of a time series of trapdoors if a \emph{simple substitution cipher} is employed as described in \Cref{sec:es_model}. However, the described adversary is not particularly sophisticated. For instance, a more sophisticated adversary incorporates the search patterns of specific search agents into the probability model described in \Cref{sec:pr_model}.

The adversaries we worry about the most are probably more clever than us. Thus, it may be asking too much to simulate them so that a \emph{reliable} confidentiality statistic can be produced. Matters are further complicated if a simple substitution cipher is not used, e.g., a homophonic encryption scheme is used to flatten the distribution of trapdoors. In this case, the confidentiality is expected to improve, but it may be difficult to quantify to what extent.

We may be able to construct a \emph{lower-bound} on confidentiality that is a function of the \emph{entropy}. The entropy of a random time series of $t$ trapdoors is given by
\begin{equation}
    \entropy\!\left(\rv{Y_1},\ldots,\rv{Y_t}\right)\, \si{bits}
\end{equation}
or on average
\begin{equation}
    \mu = \frac{\entropy\!\left(\rv{Y_1},\ldots,\rv{Y_t}\right)}{t} \, \si{bits \per trapdoor}\,.
\end{equation}
If the random time series is independently distributed, the average simplifies to
\begin{equation}
    \mu = \frac{1}{t} \sum_{j=1}^{t} \entropy\!\left(\rv{Y_j}\right) \, \si{bits \per trapdoor}
\end{equation}
and if it is also identically distributed, the average is simply given by
\begin{equation}
    \mu = \entropy\!\left(\rv{Y_1}\right) \, \si{bits \per trapdoor}\,.
\end{equation}
Consider the two extremes given by the following:
\begin{enumerate}
    \item
    \label{itm:maxentropy}
    An \emph{optimal} adversary is expected learn \emph{nothing} about the mapping from \emph{trapdoors} to \emph{plaintext keys} by observing a uniformly distributed time series.\footnote{The optimal adversary \emph{randomly} chooses a mapping.}
    
    A uniformly distributed time series of over a support set of $m$ unique trapdoor signatures has a \emph{maximum entropy} given by
    \begin{equation}
        \mu_{U} = \log_2 m \; \si{bits \per trapdoor}\,.
    \end{equation}
    
    \item
    \label{itm:minentropy}
    An \emph{optimal} adversary is expected to learn \emph{everything} about the mapping from \emph{trapdoors} to \emph{plaintext keys} by observing a degenerate time series.
    
    A degenerate time series has a \emph{minimum entropy} given by
    \begin{equation}
        \mu_{L} = 0 \; \si{bits \per trapdoor}\,.
    \end{equation}
\end{enumerate}
By \Cref{itm:maxentropy,itm:minentropy}, the average bits of entropy per trapdoor is bounded by
\begin{equation}
    0 \leq \mu \leq \log_2 m\,.
\end{equation}
We use these insights to construct an \emph{information again} measure given by the following definition.
\begin{definition}
The \emph{mean information again} with respect to a random time series of trapdoors with $m$ unique signatures is denoted by
\begin{equation}
    \lambda = \log_2 m -  \mu \; \si{bits \per trapdoor}\,,
\end{equation}
and is some constant between $0$ and $\log_2 m$.
\end{definition}
We make the following conjecture about the \emph{mean information gain}.
\begin{conjecture}
The \emph{mean information gain} $\lambda$ is related to the \emph{rate} at which the \emph{optimal} adversary learns about the mapping from \emph{trapdoors} to \emph{plaintext keys} by observing a \emph{trapdoor} in a time series. The learning rate is a monotonically increasing function of $\lambda$ and is $0$ for $\lambda = 0$.
\end{conjecture}
If we integrate the learning rate function, we derive the \emph{optimal} adversary's accuracy $p_t$ at time step $t$ as given by \Cref{eq:accuracy}. That is,
\begin{equation}
    \lambda_t = \diff{p_t}{t}\,.
\end{equation}

\subsection{Estimating entropy}
Since the probabilistic model for the random time series may not be known, we may estimate the entropy.
\begin{postulate}[Optimal compressor]
\label{post:optcomp}
The \emph{entropy} of a random time series is equivalent to the \emph{expected} bit length produced by an optimal \emph{lossless} compressor's output given the time series as input as given by
\begin{equation}
    \entropy\left(\rv{Y_1},\ldots,\rv{Y_t}\right) =
    \expectation\Biggl[
        \BL\biggl(
            \Compress^*\bigl(
                \rv{Y_1} \rv{Y_2} \cdots \rv{Y_t}
                )
            \bigr)
        \biggr)
    \Biggr]\,,
\end{equation}
where $\Compress^*$ is an lossless optimal compressor of the sequence and $\BL(x)$ is the bit length of $x$.
\end{postulate}

An estimate of entropy is given by the following definition.
\begin{definition}
Given a time series of $t$ trapdoors,
\begin{equation}
    \tau_t = \left(y_1,\ldots,y_t\right)\,,
\end{equation}
an estimator\footnote{Since this is a statistic that depends upon a random time series, the \emph{sampling distribution} may be computed using the Bootstrap method.} of the entropy is given by
\begin{equation}
    \hat{\entropy} = \BL\left(\Compress(y_1 y_2 \cdots y_t)\right)\,,
\end{equation}
where \Compress is a near-optimal compressor of the time series.
\end{definition}

It is not obvious that the entropy can map to some intuitive notion of confidentiality like the statistic derived in \Cref{}.

The adversary given by \Cref{} is \emph{optimal} if the trapdoors in the time series are drawn from a unigram language model using a simple substitution cipher. One possibility is to \emph{map} the expected gain from this distribution to the computed confidentiality, and then assume that this mapping \emph{generally} holds.
\end{document}